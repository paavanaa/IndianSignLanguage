{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNFPKDNfoYxS7hS0wQtlr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paavanaa/IndianSignLanguage/blob/main/IndianSignLanguage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Dependencies"
      ],
      "metadata": {
        "id": "lMtPVqog4lwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mzg2-2z4TbP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import os\n",
        "import time\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Mediapipe and OpenCV"
      ],
      "metadata": {
        "id": "tTA4eRNu4vYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mp_hands=mp.solutions.objectron\n",
        "mp_holistic=mp.solutions.holistic\n",
        "mp_drawing=mp.solutions.drawing_utils"
      ],
      "metadata": {
        "id": "QkI06Cix4W2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detector(image,model):\n",
        "    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    image.flags.writeable=False\n",
        "    results=model.process(image)\n",
        "    image.flags.writeable=True\n",
        "    image=cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
        "    return image,results"
      ],
      "metadata": {
        "id": "oqwgDlPk4aol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image,results):\n",
        "    #mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_CONTOURS)\n",
        "    #mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS)\n",
        "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS)"
      ],
      "metadata": {
        "id": "6i-6FhT74czl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam=cv2.VideoCapture(0)\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:\n",
        "    while cam.isOpened():\n",
        "        ret,frame=cam.read()\n",
        "        image,results=mediapipe_detector(frame,holistic)\n",
        "        #print(results)\n",
        "        draw_landmarks(image,results)\n",
        "        cv2.imshow('Python Camera',image)\n",
        "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
        "            break\n",
        "    cam.release()\n",
        "    cv2.destroyWindow('Python Camera')\n",
        "    cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "m-FcdEUl4gde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(results.pose_landmarks.landmark)"
      ],
      "metadata": {
        "id": "tsh7ZxAj4ie8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Data to NP Array"
      ],
      "metadata": {
        "id": "yLvtlryJ44Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
        "#face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
        "lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)"
      ],
      "metadata": {
        "id": "NcEYM8V147Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
        "    #pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "    return np.concatenate([lh, rh])"
      ],
      "metadata": {
        "id": "4GUn0egJ4-Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_keypoints(results)"
      ],
      "metadata": {
        "id": "g4tNgC3y5AUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Save Data to file /Making data for the model"
      ],
      "metadata": {
        "id": "2c2hoB1y5JFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = os.path.join('Final_Data')\n",
        "no_of_sequences=40\n",
        "sequence_length=45\n"
      ],
      "metadata": {
        "id": "ZOY0CK995Jq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions=np.array(['Hi','I Love You','How Are You','Good Morning','What Is Your Name','Sorry','Thank You','India'])\n",
        "for action in actions:\n",
        "    for sequence in range(no_of_sequences):\n",
        "        try:\n",
        "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "id": "PAbzVwii5Mp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    for action in actions:\n",
        "        for sequence in range(no_of_sequences):\n",
        "            for frame_no in range(sequence_length):\n",
        "                ret, frame = cam.read()\n",
        "                image, results = mediapipe_detector(frame, holistic)\n",
        "                draw_landmarks(image, results)\n",
        "\n",
        "                # Convert 0-based index to 1-based for display\n",
        "                display_sequence = sequence + 1\n",
        "                display_frame = frame_no + 1\n",
        "\n",
        "                # Display start of collection message\n",
        "                if frame_no == 0:\n",
        "                    cv2.putText(image, '--- STARTING COLLECTION ---', (50, 100),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
        "                    cv2.putText(image, f'Collecting frames for \"{action}\" | Video #{display_sequence}', (50, 150),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "                    cv2.imshow('Python Camera', image)\n",
        "                    cv2.waitKey(2000)  # Pause for 2 seconds\n",
        "                else:\n",
        "                    cv2.putText(image, f'Collecting \"{action}\" | Video #{display_sequence} | Frame {display_frame}', (50, 100),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "                    cv2.imshow('Python Camera', image)\n",
        "\n",
        "                # Save keypoints\n",
        "                keypoints = extract_keypoints(results)\n",
        "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_no))\n",
        "                np.save(npy_path, keypoints)\n",
        "\n",
        "                # Break on 'q'\n",
        "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "                    break\n",
        "\n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)\n"
      ],
      "metadata": {
        "id": "iz4I-Jtj5NZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cam.release()\n",
        "cv2.destroyAllWindows('Python Camera')\n",
        "cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "RxE-RJSY5Qxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Labeling the data using SKlearn"
      ],
      "metadata": {
        "id": "1ri0WzRz5TJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "r92depIC5XSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map={label: num for num,label in enumerate(actions)}\n",
        "print(label_map)"
      ],
      "metadata": {
        "id": "QA4K1O3M5ZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences, labels = [], []\n",
        "for action in actions:\n",
        "    for sequence in range(no_of_sequences):\n",
        "        window = []\n",
        "        for frame_num in range(sequence_length):\n",
        "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
        "            window.append(res)\n",
        "        sequences.append(window)\n",
        "        labels.append(label_map[action])"
      ],
      "metadata": {
        "id": "pa7EpwHB5bRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(sequences)"
      ],
      "metadata": {
        "id": "KD7dC-EF5e1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "NZ9GyIAr5i1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(labels).astype(int)"
      ],
      "metadata": {
        "id": "b0SLe3i35kbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.05)"
      ],
      "metadata": {
        "id": "15D4bl9-5mt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "JEj3TG1b5o23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Making Model"
      ],
      "metadata": {
        "id": "2aHt-l4i5rvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.is_gpu_available())"
      ],
      "metadata": {
        "id": "cKlXbCg15u8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ],
      "metadata": {
        "id": "UGjHB6rL5ySL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "RY9hir0i50AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# LSTM layers with a moderate number of units, using Batch Normalization\n",
        "model.add(LSTM(64, return_sequences=True, input_shape=(30, 258)))  # 30 frames, 258 features\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(128,return_sequences=True))  # No return_sequences since we're only interested in final prediction\n",
        "model.add(BatchNormalization())\n",
        "model.add(LSTM(64))\n",
        "model.add(BatchNormalization())\n",
        "# Dense layers for classification\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))  # Output layer for 4 gesture categories\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "gPYtapCk51pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, BatchNormalization, Bidirectional\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# 2 DNN Layers\n",
        "#model.add(Dense(64, activation='relu', input_shape=(45, 1662)))\n",
        "model.add(Dense(128, activation='relu',input_shape=(45, 126)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "# 3 Bidirectional GRU Layers\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "#model.add(BatchNormalization())  # Between GRU layers\n",
        "\n",
        "model.add(GRU(64, return_sequences=True))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(GRU(32))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "# Final Dense Layer for Classification\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dense(8, activation='softmax'))  # Change to 8 if you have 8 classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "dX_mFSVl54DC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'],)"
      ],
      "metadata": {
        "id": "X77zxzZ556J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=50,batch_size=4,callbacks=[tb_callback])"
      ],
      "metadata": {
        "id": "0lDIPwCi58D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model"
      ],
      "metadata": {
        "id": "9iF92F_d5-Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=tf.keras.models.load_model(\"IndianSignLang.h5\")"
      ],
      "metadata": {
        "id": "XxdtNKI_5-93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "IGvrSOzh6Es3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = model.predict(X_test)"
      ],
      "metadata": {
        "id": "4BzWiwW46IJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(16):\n",
        "    print(actions[np.argmax(res[i])],end=\"-->\")\n",
        "    print(actions[np.argmax(y_test[i])])"
      ],
      "metadata": {
        "id": "ts8l0fCy6J34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = model.evaluate(X_test, y_test)\n",
        "#print(f\"Test Loss: {eval_result[0]}, Test Accuracy: {eval_result[1]}\")"
      ],
      "metadata": {
        "id": "iWyt2Dsc6MEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(16):\n",
        "    print(actions[np.argmax(y_test[i])])\n",
        "\n"
      ],
      "metadata": {
        "id": "D_y8L-op6OOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"IndianSignLang.h5\")"
      ],
      "metadata": {
        "id": "7gMAkEdX6OuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REAL TIME"
      ],
      "metadata": {
        "id": "RggC2EhA6e4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\n",
        "    (245, 117, 16),  # Orange\n",
        "    (117, 245, 16),  # Green\n",
        "    (16, 117, 245),  # Blue\n",
        "    (245, 16, 117),  # Pink\n",
        "    (117, 16, 245),  # Purple\n",
        "    (16, 245, 117),  # Teal\n",
        "    (245, 245, 16),  # Yellow\n",
        "    (117, 16, 245),  # Purple\n",
        "]\n",
        "def prob_viz_top(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "    top_idx = np.argmax(res)  # Index of the highest probability\n",
        "    top_prob = res[top_idx]  # Value of the highest probability\n",
        "\n",
        "    # Display the top prediction and its probability\n",
        "    cv2.rectangle(output_frame, (0, 60), (int(top_prob * 300), 100), colors[top_idx], -1)\n",
        "    cv2.putText(output_frame, f\"{actions[top_idx]}: {top_prob:.2f}\", (10, 90),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    return output_frame\n"
      ],
      "metadata": {
        "id": "JR9AIrrg6gC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\n",
        "    (245, 117, 16),  # Orange\n",
        "    (117, 245, 16),  # Green\n",
        "    (16, 117, 245),  # Blue\n",
        "    (245, 16, 117),  # Pink\n",
        "    (117, 16, 245),  # Purple\n",
        "    (16, 245, 117),  # Teal\n",
        "    (245, 245, 16),  # Yellow\n",
        "    (117, 16, 245),  # Purple\n",
        "]\n",
        "\n",
        "def prob_viz(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "    for num, prob in enumerate(res):\n",
        "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
        "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "    return output_frame"
      ],
      "metadata": {
        "id": "XNmxmHPi6iKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.5\n",
        "\n",
        "cam = cv2.VideoCapture(0)\n",
        "# Set mediapipe model\n",
        "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "    while cam.isOpened():\n",
        "\n",
        "        # Read feed\n",
        "        ret, frame = cam.read()\n",
        "\n",
        "        # Make detections\n",
        "        image, results = mediapipe_detector(frame, holistic)\n",
        "        #print(results)\n",
        "\n",
        "        # Draw landmarks\n",
        "        draw_landmarks(image, results)\n",
        "\n",
        "        # 2. Prediction logic\n",
        "        keypoints = extract_keypoints(results)\n",
        "        sequence.append(keypoints)\n",
        "        sequence = sequence[-45:]\n",
        "\n",
        "        if len(sequence) == 45:\n",
        "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
        "            #print(actions[np.argmax(res)])\n",
        "            predictions.append(np.argmax(res))\n",
        "\n",
        "\n",
        "        #3. Viz logic\n",
        "            if np.unique(predictions[-10:])[0]==np.argmax(res):\n",
        "                if res[np.argmax(res)] > threshold:\n",
        "\n",
        "                    if len(sentence) > 0:\n",
        "                        if actions[np.argmax(res)] != sentence[-1]:\n",
        "                            sentence.append(actions[np.argmax(res)])\n",
        "                    else:\n",
        "                        sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "            if len(sentence) > 5:\n",
        "                sentence = sentence[-5:]\n",
        "\n",
        "            # Viz probabilities\n",
        "            image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
        "        cv2.putText(image, ' '.join(sentence), (3,30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "        # Show to screen\n",
        "        cv2.imshow('OpenCV Feed', image)\n",
        "\n",
        "        # Break gracefully\n",
        "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "            break\n",
        "    cam.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "WE1cfGRL6kGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "cv2.waitKey(1)"
      ],
      "metadata": {
        "id": "-7-xIhng6mCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model('IndianSignLang.h5')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('gesture_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "zhnbGAig6n4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NAc4ywH_6rK4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}